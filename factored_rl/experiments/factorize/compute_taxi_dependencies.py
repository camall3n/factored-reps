from collections import OrderedDict

import matplotlib.pyplot as plt
import numpy as np
from omegaconf import OmegaConf
import pandas as pd
import pickle
from scipy.special import xlogy
import seaborn as sns
import torch

from factored_rl import configs
from factored_rl.test.utils import get_config
from visgrid.envs.gridworld import GridworldEnv
from factored_rl.experiments.common import initialize_env, initialize_model
from factored_rl.experiments.factorize.run import initialize_dataloader

np.set_printoptions(precision=4, suppress=True)

#%%
cfg = get_config([
    "experiment=pytest",
    "timestamp=false",
    "env=taxi",
    "env.depot_dropoff_only=false",
    "transform=identity",
    "transform.noise=false",
])
env = initialize_env(cfg, cfg.seed).unwrapped

data = []
for _ in range(1000):
    state, info = env.reset()
    action = env.action_space.sample()
    next_state, _, _, _, info = env.step(action)
    experience = {
        'state': state,
        'action': action,
        'next_state': next_state,
        'effect': next_state - state,
    }
    experience.update({f"s_{i}": s for i, s in enumerate(state)})
    experience.update({f"s'_{i}": sp for i, sp in enumerate(next_state)})
    data.append(experience)

df = pd.DataFrame(data)

#%%

def mi(df, col1, col2):
    """
    Compute discrete mutual information (in bits) for two columns of a pandas dataframe

    Generated by GPT4
    """
    joint_counts = pd.crosstab(df[col1], df[col2]).values
    joint_prob = joint_counts / joint_counts.sum()

    col1_prob = joint_counts.sum(axis=1, keepdims=True) / joint_counts.sum()
    col2_prob = joint_counts.sum(axis=0, keepdims=True) / joint_counts.sum()

    mutual_info = np.sum(xlogy(joint_prob, joint_prob / (col1_prob @ col2_prob))) / np.log(2)

    return mutual_info

def test_mi():
    """
    Generated by GPT4
    """
    # Test 1: Perfect correlation; MI = entropy(col1) = log2(2)
    data1 = {'A': [1, 0, 1, 0, 1, 1, 0, 0], 'B': [1, 0, 1, 0, 1, 1, 0, 0]}
    df1 = pd.DataFrame(data1)
    mi1 = mi(df1, 'A', 'B')
    assert np.isclose(mi1, np.log2(2), atol=1e-6)

    # Test 2: No correlation; MI = 0
    data2 = {'A': [1, 0, 1, 0, 1, 1, 0, 0], 'B': [0, 1, 0, 1, 1, 1, 0, 0]}
    df2 = pd.DataFrame(data2)
    mi2 = mi(df2, 'A', 'B')
    assert np.isclose(mi2, 0, atol=1e-6)

    # Test 3: Identical columns with more than two unique values
    # MI = entropy(col1) = log2(3)
    data3 = {'A': [1, 2, 3, 1, 2, 3], 'B': [1, 2, 3, 1, 2, 3]}
    df3 = pd.DataFrame(data3)
    mi3 = mi(df3, 'A', 'B')
    assert np.isclose(mi3, np.log2(3), atol=1e-6)

test_mi()

def calculate_mi_matrix(df):
    """
    Calculate the mutual information between pairs of columns in a dataframe

    Generated by GPT4
    """
    n = len(state)
    current_col = [f"s_{i}" for i in range(n)]
    next_col = [f"s'_{i}" for i in range(n)]

    mi_matrix = np.zeros((n, n))

    for i in range(n):
        for j in range(i, n):
            x = mi(df, current_col[i], next_col[j])
            mi_matrix[i, j] = x
            mi_matrix[j, i] = x

    return mi_matrix

m = calculate_mi_matrix(df)

np.power(2, m).round(1)
