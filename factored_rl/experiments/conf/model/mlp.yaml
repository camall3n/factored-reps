defaults:
  - base_model

name: mlp
lib: factored_rl
arch:
  type: enc
  encoder: mlp
flatten_input: true
mlp:
  n_hidden_layers: 1
  n_units_per_layer: 64
  activation:
    - _target_: torch.nn.Tanh
  final_activation: null
n_latent_dims: 10
qnet:
  n_hidden_layers: 1
  n_units_per_layer: 64
  activation:
    - _target_: torch.nn.ReLU
  final_activation: null
