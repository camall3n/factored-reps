defaults:
  - base_trainer
  - _self_

name: full
batch_size: 128
learning_rate: 0.001
log_every_n_steps: 50
max_steps: 2000
num_dataloader_workers: 2
optimizer:
  _target_: torch.optim.AdamW
  _partial_: true
overfit_batches: 0
persistent_workers: true
quick: false
